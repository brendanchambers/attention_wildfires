{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# crawl outward to build a keyword list for fmri papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendanchambers/.conda/envs/embedding-base/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import mysql.connector as mysql\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import Row, StructType, StructField, IntegerType, StringType\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_locally = False # set this switch for RCC vs laptop mode\n",
    "\n",
    "if run_locally:\n",
    "    path2bin = '~/Dropbox/Data/w2v/'\n",
    "    pubmed_w2v = path2bin + 'PubMed-and-PMC-w2v.bin'\n",
    "else:\n",
    "    path2bin = '/project2/jevans/brendan/w2v/pretrained/'\n",
    "    #pubmed_w2v = path2bin + 'PubMed-w2v.bin'\n",
    "    #pubmed_w2v = path2bin + 'PubMed-and-PMC-w2v.bin'\n",
    "    pubmed_w2v = path2bin + 'wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained word vectors (trained on biomedical data)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    pubmed_w2v, binary=True)\n",
    "\n",
    "# prepare a tokenizer to process the abstracts\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load(\"en_core_sci_sm\")  # load scispacy\n",
    "#tokenizer = nlp.Defaults.create_tokenizer(nlp)  # ideally would want to use the scispacy version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fmri']\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words2try = ['fmri','bold']\n",
    "keywords = []\n",
    "for word in words2try:\n",
    "    \n",
    "    try:\n",
    "        v = model.word_vec(word)\n",
    "        keywords.append(word)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "print(keywords)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# get dimensionality\n",
    "D = np.shape(model.word_vec('cell'))[0]\n",
    "print(D)\n",
    "\n",
    "\n",
    "# find neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pubmed word2vec binary file\n",
    "\n",
    "# check for the entry 'fmri'\n",
    "\n",
    "# find neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing spark\n",
      "[('spark.repl.local.jars', 'file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar'), ('spark.driver.memory', '26G'), ('spark.driver.port', '34140'), ('spark.driver.host', 'midway2-0131.rcc.local'), ('spark.jars', '/home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar'), ('spark.executor.id', 'driver'), ('spark.app.name', 'pyspark-shell'), ('spark.app.id', 'local-1569536401180'), ('spark.rdd.compress', 'True'), ('spark.driver.extraClassPath', 'file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "# load the papers using pyspark\n",
    "\n",
    "# filter on the text\n",
    "'''\n",
    "SUBMIT_ARGS = \"--driver-class-path file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar --jars file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "\n",
    "db_name = 'test_pubmed'  # db name collisons? https://stackoverflow.com/questions/14011968/user-cant-access-a-database\n",
    "url = \"jdbc:mysql://localhost:3306/{}?useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=America/Chicago\".format(db_name)  # mysql runs on port 3306\n",
    "client_config = {'unix_socket':'/home/brendanchambers/.sql.sock',\n",
    "                'database': db_name}  # for python connector\n",
    "\n",
    "print('initializing spark')\n",
    "# init spark\n",
    "conf = SparkConf()\n",
    "conf = (conf.setMaster('local[*]')\n",
    "       .set('spark.driver.memory','26G')\n",
    "       .set(\"spark.jars\", \"/home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar\"))        \n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "#sc.addJar('home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar')  # temp\n",
    "spark = SparkSession(sc)  # don't need this for vanilla RDDs\n",
    "\n",
    "print(sc._conf.getAll())\n",
    "'''\n",
    "\n",
    "# for mysql connector\n",
    "client_config = {'unix_socket':'/home/brendanchambers/.sql.sock',\n",
    "                            'database':'test_pubmed'}\n",
    "                            #'use_pure':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try filtering first (this isn't working - sql syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Unread result found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-429b76507ba4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/embedding-base/lib/python3.7/site-packages/mysql/connector/cursor_cext.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unread_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warnings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cnx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/embedding-base/lib/python3.7/site-packages/mysql/connector/connection_cext.py\u001b[0m in \u001b[0;36mhandle_unread_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsume_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munread_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInternalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unread result found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m: Unread result found"
     ]
    }
   ],
   "source": [
    "\n",
    "# step 2 build a list of papers\n",
    "\n",
    "# get all papers post-1985\n",
    "cutoffYear = 1985\n",
    "keyword = 'fmri'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "db = mysql.connect(**client_config)\n",
    "sql = '''SELECT A.pmid, A.title, A.abstract, M.year, M.journal\n",
    "            FROM metadata as M\n",
    "            JOIN abstracts as A\n",
    "            ON M.pmid = A.pmid\n",
    "            WHERE \n",
    "              (M.year > {}) AND\n",
    "              (A.abstract LIKE '%{}%')'''.format(cutoffYear, keyword)\n",
    "\n",
    "cursor = db.cursor(buffered=False)\n",
    "cursor.execute(sql)\n",
    "\n",
    "subset = []\n",
    "for i,row in enumerate(cursor):\n",
    "    \n",
    "    abstract = row[2]\n",
    "    \n",
    "    if keyword in abstract:\n",
    "        print(abstract)\n",
    "        \n",
    "    subset.append(row)\n",
    "    \n",
    "    if i > 1:\n",
    "        break\n",
    "\n",
    "cursor.close()\n",
    "db.commit()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"journals enumerated in {} s\".format(end_time - start_time))\n",
    "\n",
    "\n",
    "# filter on keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try grabbing all and then filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pyspark.sql.functions as psf\\nkeyword=\\'fmri\\'\\n\\n\\nsql = \"(SELECT * FROM abstracts) AS t\"\\nprint(sql)\\n\\nstart_time = time.time()\\ndf = spark.read.format(\\'jdbc\\').option(\"url\", url)                              .option(\"dbtable\", sql)                              .load().repartition(10)\\ndf.filter(psf.lower(df.abstract).rlike(keyword))\\n\\n# test\\ntest = df.take(1)\\n\\nend_time = time.time()\\nprint(\"dataframe loaded in {} s\".format(end_time - start_time))\\n\\nprint(df.rdd.getNumPartitions())\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pyspark.sql.functions as psf\n",
    "keyword='fmri'\n",
    "\n",
    "\n",
    "sql = \"(SELECT * FROM abstracts) AS t\"\n",
    "print(sql)\n",
    "\n",
    "start_time = time.time()\n",
    "df = spark.read.format('jdbc').option(\"url\", url)\\\n",
    "                              .option(\"dbtable\", sql)\\\n",
    "                              .load().repartition(10)\n",
    "df.filter(psf.lower(df.abstract).rlike(keyword))\n",
    "\n",
    "# test\n",
    "test = df.take(1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"dataframe loaded in {} s\".format(end_time - start_time))\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many articles in this keyword-defined core?\n",
    "# save output to csv file, db table, or json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3  build the neighborhood of cited and citing work\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4 map retractions into the network, and score nodes based on closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
