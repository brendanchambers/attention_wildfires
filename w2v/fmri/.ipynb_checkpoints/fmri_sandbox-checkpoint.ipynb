{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# crawl outward to build a keyword list for fmri papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendanchambers/.conda/envs/embedding-base/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import mysql.connector as mysql\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import Row, StructType, StructField, IntegerType, StringType\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_locally = False # set this switch for RCC vs laptop mode\n",
    "\n",
    "if run_locally:\n",
    "    path2bin = '~/Dropbox/Data/w2v/'\n",
    "    pubmed_w2v = path2bin + 'PubMed-and-PMC-w2v.bin'\n",
    "else:\n",
    "    path2bin = '/project2/jevans/brendan/w2v/pretrained/'\n",
    "    #pubmed_w2v = path2bin + 'PubMed-w2v.bin'\n",
    "    #pubmed_w2v = path2bin + 'PubMed-and-PMC-w2v.bin'\n",
    "    pubmed_w2v = path2bin + 'wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained word vectors (trained on biomedical data)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    pubmed_w2v, binary=True)\n",
    "\n",
    "# prepare a tokenizer to process the abstracts\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load(\"en_core_sci_sm\")  # load scispacy\n",
    "#tokenizer = nlp.Defaults.create_tokenizer(nlp)  # ideally would want to use the scispacy version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fmri']\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "words2try = ['fmri','bold']\n",
    "keywords = []\n",
    "for word in words2try:\n",
    "    \n",
    "    try:\n",
    "        v = model.word_vec(word)\n",
    "        keywords.append(word)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "print(keywords)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# get dimensionality\n",
    "D = np.shape(model.word_vec('cell'))[0]\n",
    "print(D)\n",
    "\n",
    "\n",
    "# find neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pubmed word2vec binary file\n",
    "\n",
    "# check for the entry 'fmri'\n",
    "\n",
    "# find neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing spark\n",
      "[('spark.repl.local.jars', 'file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar'), ('spark.driver.memory', '26G'), ('spark.driver.port', '34140'), ('spark.driver.host', 'midway2-0131.rcc.local'), ('spark.jars', '/home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar'), ('spark.executor.id', 'driver'), ('spark.app.name', 'pyspark-shell'), ('spark.app.id', 'local-1569536401180'), ('spark.rdd.compress', 'True'), ('spark.driver.extraClassPath', 'file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "# load the papers using pyspark\n",
    "\n",
    "# filter on the text\n",
    "'''\n",
    "SUBMIT_ARGS = \"--driver-class-path file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar --jars file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "\n",
    "db_name = 'test_pubmed'  # db name collisons? https://stackoverflow.com/questions/14011968/user-cant-access-a-database\n",
    "url = \"jdbc:mysql://localhost:3306/{}?useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=America/Chicago\".format(db_name)  # mysql runs on port 3306\n",
    "client_config = {'unix_socket':'/home/brendanchambers/.sql.sock',\n",
    "                'database': db_name}  # for python connector\n",
    "\n",
    "print('initializing spark')\n",
    "# init spark\n",
    "conf = SparkConf()\n",
    "conf = (conf.setMaster('local[*]')\n",
    "       .set('spark.driver.memory','26G')\n",
    "       .set(\"spark.jars\", \"/home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar\"))        \n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "#sc.addJar('home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar')  # temp\n",
    "spark = SparkSession(sc)  # don't need this for vanilla RDDs\n",
    "\n",
    "print(sc._conf.getAll())\n",
    "'''\n",
    "\n",
    "# for mysql connector\n",
    "client_config = {'unix_socket':'/home/brendanchambers/.sql.sock',\n",
    "                            'database':'test_pubmed'}\n",
    "                            #'use_pure':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try filtering first (this isn't working - sql syntax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step 2 build a list of papers\n",
    "\n",
    "# get all papers post-1985\n",
    "cutoffYear = 1985\n",
    "keyword = 'fmri'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "db = mysql.connect(**client_config)\n",
    "sql = '''SELECT A.pmid, A.title, A.abstract, M.year, M.journal\n",
    "            FROM metadata as M\n",
    "            JOIN abstracts as A\n",
    "            ON M.pmid = A.pmid\n",
    "            WHERE \n",
    "              (M.year > {}) AND\n",
    "              ({} in A.abstract)'''.format(cutoffYear, keyword)\n",
    "\n",
    "cursor = db.cursor(buffered=False)\n",
    "cursor.execute(sql)\n",
    "\n",
    "subset = []\n",
    "for i,row in enumerate(cursor):\n",
    "    \n",
    "    abstract = row[2]\n",
    "    \n",
    "    if keyword in abstract:\n",
    "        print(abstract)\n",
    "        \n",
    "    subset.append(row)\n",
    "    \n",
    "    if i > 1:\n",
    "        break\n",
    "\n",
    "cursor.close()\n",
    "db.commit()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"journals enumerated in {} s\".format(end_time - start_time))\n",
    "\n",
    "\n",
    "# filter on keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try grabbing all and then filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pyspark.sql.functions as psf\\nkeyword=\\'fmri\\'\\n\\n\\nsql = \"(SELECT * FROM abstracts) AS t\"\\nprint(sql)\\n\\nstart_time = time.time()\\ndf = spark.read.format(\\'jdbc\\').option(\"url\", url)                              .option(\"dbtable\", sql)                              .load().repartition(10)\\ndf.filter(psf.lower(df.abstract).rlike(keyword))\\n\\n# test\\ntest = df.take(1)\\n\\nend_time = time.time()\\nprint(\"dataframe loaded in {} s\".format(end_time - start_time))\\n\\nprint(df.rdd.getNumPartitions())\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pyspark.sql.functions as psf\n",
    "keyword='fmri'\n",
    "\n",
    "\n",
    "sql = \"(SELECT * FROM abstracts) AS t\"\n",
    "print(sql)\n",
    "\n",
    "start_time = time.time()\n",
    "df = spark.read.format('jdbc').option(\"url\", url)\\\n",
    "                              .option(\"dbtable\", sql)\\\n",
    "                              .load().repartition(10)\n",
    "df.filter(psf.lower(df.abstract).rlike(keyword))\n",
    "\n",
    "# test\n",
    "test = df.take(1)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"dataframe loaded in {} s\".format(end_time - start_time))\n",
    "\n",
    "print(df.rdd.getNumPartitions())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many articles in this keyword-defined core?\n",
    "# save output to csv file, db table, or json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3  build the neighborhood of cited and citing work\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4 map retractions into the network, and score nodes based on closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
