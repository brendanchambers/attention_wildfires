{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendanchambers/.conda/envs/embedding-base/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "# concatenate words in a few examples, using word2vec\n",
    "\n",
    "# look at cross-validation (cosine distance method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2bin = '/project2/jevans/brendan/w2v/pretrained/'\n",
    "#pubmed_w2v = path2bin + 'PubMed-w2v.bin'\n",
    "#pubmed_w2v = path2bin + 'PubMed-and-PMC-w2v.bin'\n",
    "pubmed_w2v = path2bin + 'wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "\n",
    "validation_filepath = '/project2/jevans/brendan/pubmed_data_processing/validation_sets/'\n",
    "validation_filename = validation_filepath + 'jneurophysiol_vs_neuroimage.csv'\n",
    "embedding_subchunk_size = 10000  # do 10000 abstracts at a time\n",
    "\n",
    "embedding_target_path = '/project2/jevans/brendan/pubmed_data_processing/validation_sets/jneurophysiol_vs_neuroimage_results/'\n",
    "embedding_flavor = 'w2v_wikipedia_pubmed_pmc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained word vectors (trained on biomedical data)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    pubmed_w2v, binary=True)\n",
    "\n",
    "# prepare a tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#tokenizer = nlp.Defaults.create_tokenizer(nlp)  # ideally would want to use the scispacy version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# get the dimensionality\n",
    "D = np.shape(model.word_vec('test'))[0]\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "header: ['pmid', 'title', 'abstract']\n",
      "embedding subchunk number 0.0...\n",
      "elapsed (s): 0.008095502853393555\n",
      "processing batch of abstracts...\n",
      "writing embeddings to csv file...\n",
      "elapsed: 0.004990816116333008\n",
      "embedding subchunk number 1.0...\n",
      "elapsed (s): 0.1894526481628418\n",
      "processing batch of abstracts...\n",
      "writing embeddings to csv file...\n",
      "elapsed: 1.911231279373169\n",
      "embedding subchunk number 2.0...\n",
      "elapsed (s): 2.072589635848999\n",
      "processing batch of abstracts...\n",
      "writing embeddings to csv file...\n",
      "elapsed: 1.8925697803497314\n",
      "embedding subchunk number 3.0...\n",
      "elapsed (s): 2.058314085006714\n",
      "processing batch of abstracts...\n",
      "writing embeddings to csv file...\n",
      "elapsed: 1.9412379264831543\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "data = []\n",
    "with open(validation_filename) as f:\n",
    "\n",
    "    csvreader = csv.reader(f, delimiter=' ')\n",
    "    header = next(csvreader) # skip header\n",
    "    print(\"header: {}\".format(header))\n",
    "\n",
    "    for idx, row in enumerate(csvreader):\n",
    "        data.append(row)\n",
    "\n",
    "        if idx % embedding_subchunk_size == 0:\n",
    "\n",
    "            print('embedding subchunk number {}...'.format(idx / embedding_subchunk_size))\n",
    "\n",
    "            # perform embedding:\n",
    "            ###########################\n",
    "            # remove rows with no text\n",
    "            del_idxs = []\n",
    "            for rowidx, row in enumerate(data):  # don't embed empty text\n",
    "                if row[2] == '' and row[1] == '':\n",
    "                    del_idxs.append(rowidx)\n",
    "            for delidx in sorted(del_idxs, reverse=True):\n",
    "                del data[delidx]\n",
    "\n",
    "            abstracts = [t[1] + '. ' + t[2] for t in data]  # (pmid, title, abstract)\n",
    "\n",
    "            end_time = time.time()\n",
    "            print('elapsed (s): {}'.format(end_time - start_time))\n",
    "\n",
    "            \n",
    "            ##########   Process Batch of Abstracts\n",
    "            print('processing batch of abstracts...')\n",
    "            entries = []\n",
    "            for docidx, doc in enumerate(nlp.tokenizer.pipe(abstracts)):\n",
    "                embedding = np.zeros(D)\n",
    "                token_count = 1\n",
    "                err = []\n",
    "                for i_token, token in enumerate(doc):\n",
    "                    ############ control the representation here #############\n",
    "                    try:\n",
    "                        embedding += model.word_vec(token.lower_)\n",
    "                        token_count += 1\n",
    "                    except Exception as e:  # for words outside of vocabulary\n",
    "                        err.append(e)\n",
    "\n",
    "                    ###########################################################\n",
    "                embedding = (1.0 / token_count) * embedding  # norm\n",
    "                pmid = data[docidx][0]\n",
    "                entries.append((pmid, embedding.tolist(),))            \n",
    "            \n",
    "            ########\n",
    "            # write to file\n",
    "\n",
    "            print('writing embeddings to csv file...')\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with open(embedding_target_path + embedding_flavor + '.csv', 'a') as f:\n",
    "                csv_out = csv.writer(f, delimiter=' ')\n",
    "                for row_ in entries:\n",
    "                    csv_out.writerow(row_)\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(\"elapsed: {}\".format(end_time - start_time))\n",
    "\n",
    "            # reset data\n",
    "            data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare word-level differences by keeping a dictionary for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
