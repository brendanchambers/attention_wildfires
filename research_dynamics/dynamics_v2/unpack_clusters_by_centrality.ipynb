{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate lists of words to explain the clusters\n",
    "\n",
    "### Feb 8 2020, Brendan Chambers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import pymysql\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import networkx as nx\n",
    "\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from pmids2vec import pmids2vec\n",
    "from pmids2corpus import pmids2corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### control params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path2clusteredPMIDs = 'data_processing_feb2020/abstracts_2018_250k.json'\n",
    "#model_dir = 'data_processing_feb2020/titles_2018_250k'\n",
    "\n",
    "path2clusteredPMIDs = 'data_processing_feb2020/abstracts_2018_250k.json'\n",
    "model_dir = 'data_processing_feb2020/titles_2018_250k'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_processing_feb2020/abstracts_2018_250k.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-66750b06307f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## number of samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2clusteredPMIDs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mN_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pmids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_processing_feb2020/abstracts_2018_250k.json'"
     ]
    }
   ],
   "source": [
    "## number of samples\n",
    "with open(path2clusteredPMIDs, 'r') as f:\n",
    "    data = json.load(f)\n",
    "N_samples = len(data['pmids'])\n",
    "\n",
    "dir_files = os.listdir(model_dir)\n",
    "\n",
    "samples = {}\n",
    "for i_sample in range(N_samples):\n",
    "    model_names = []\n",
    "    for f in dir_files:\n",
    "        if '.model' in f:\n",
    "            if '_{}_'.format(i_sample) in f:\n",
    "                modelpath = os.path.join(model_dir, f)\n",
    "                model_names.append(modelpath)\n",
    "\n",
    "    model_names = sorted(model_names)  # sort by order of sample, order of cluster\n",
    "    short_names = []\n",
    "    for path in model_names:\n",
    "        print(path)\n",
    "        \n",
    "        short_name = str.split(path,'/')[-1]\n",
    "        print(short_name)\n",
    "        short_names.append(short_name)\n",
    "    print('-----------')\n",
    "        \n",
    "    samples[i_sample] = {}\n",
    "    samples[i_sample]['model_names'] = model_names\n",
    "    samples[i_sample]['short_names'] = short_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in word2vec models trained on each coarse cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp, recreate the list of file names without re-training the w2v models\n",
    "\n",
    "for i_sample in range(N_samples):\n",
    "    print(\"sample {}\".format(i_sample))\n",
    "    print('---------')\n",
    "    \n",
    "    #models = {}\n",
    "    for i_name, model_name in enumerate(samples[i_sample]['model_names']):\n",
    "\n",
    "        short_name = samples[i_sample]['short_names'][i_name]\n",
    "        print(short_name)\n",
    "\n",
    "        model = Word2Vec.load(model_name)\n",
    "        samples[i_sample][short_name] = {}\n",
    "        samples[i_sample][short_name]['w2v_model'] = model\n",
    "        #models[short_name] = model\n",
    "        print('{} words '.format(len(model.wv.vocab)))  # length of the vocab dict\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-plot the clusters to verify which is which\n",
    "    \n",
    "with open(path2clusteredPMIDs,'r') as f:\n",
    "    data = json.load(f)\n",
    "    pmids = data['pmids']\n",
    "    summary_coords = data['summary_coords']\n",
    "    \n",
    "sample_IDs = [k for k in pmids.keys()]\n",
    "\n",
    "for i_sample, sample in enumerate(sample_IDs):  # todo automate number of samples detection\n",
    "\n",
    "    clusters = [k for k in pmids[sample].keys()]\n",
    "    N_COL, N_ROW = len(clusters), 1\n",
    "    fig, axs = plt.subplots(N_ROW, N_COL, sharex='all', sharey='all',\n",
    "                               figsize=(6,2))\n",
    "\n",
    "    for i_cluster, cluster in enumerate(clusters):\n",
    "        print('number of pmids: {}'.format(len(summary_coords[sample][cluster])))\n",
    "        xx = [p[0] for p in summary_coords[sample][cluster]]\n",
    "        yy = [p[1] for p in summary_coords[sample][cluster]]\n",
    "\n",
    "        axs[int(i_cluster)].scatter(xx,\n",
    "                               yy,\n",
    "                              marker='.')\n",
    "        axs[int(i_cluster)].set_title('cluster {}'.format(i_cluster))\n",
    "        \n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a networkx graph for each word similarity network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo use unique vocabularies for starters\n",
    "\n",
    "for i_sample, sample in enumerate(sample_IDs):\n",
    "\n",
    "    # todo should we match vocabulary sizes?\n",
    "    clusters = samples[i_sample]['model_names']\n",
    "    N_COL, N_ROW = 2, len(clusters)\n",
    "    fig, axs = plt.subplots(N_ROW, N_COL, sharex='none', sharey='none',\n",
    "                               figsize=(6,9))\n",
    "\n",
    "    nx_graphs = []\n",
    "    for idx, model_name in enumerate(samples[i_sample]['short_names']):\n",
    "\n",
    "        model = samples[i_sample][model_name]['w2v_model']\n",
    "\n",
    "        # the number of nodes is small so we can do this with naive mat mul, nothing clever needed\n",
    "\n",
    "        D = np.shape(model.wv['data'])[0] # dimensionality of a sample word, any word will do\n",
    "        print(\"number of dimensions D: {}\".format(D))\n",
    "\n",
    "        embedding_samples = np.zeros( (len(model.wv.vocab), D) )\n",
    "\n",
    "        # collect samples\n",
    "        for i_word, word in enumerate(model.wv.vocab):\n",
    "            embedding_samples[i_word,:] = model.wv[word] \n",
    "        # demean\n",
    "        for i_word, word in enumerate(model.wv.vocab):\n",
    "            embedding_samples[i_word,:] -= np.mean(embedding_samples,0)\n",
    "        # normalize all vectors to the hypersphere\n",
    "        for i_row,row in enumerate(embedding_samples):\n",
    "            embedding_samples[i_row,:] /= np.linalg.norm(row,2)\n",
    "        # compute edge weights via cosine similarity (projection of normalized vectors)\n",
    "        W = np.matmul(embedding_samples, embedding_samples.T)\n",
    "        # threshold and clean adjmat\n",
    "        np.fill_diagonal(W, 0)  # no self-loops\n",
    "        P_thresh = 90\n",
    "        thresh_high = np.percentile(W.flatten(), P_thresh)\n",
    "        boolean_mask = W < thresh_high\n",
    "        W[boolean_mask] = 0 # \n",
    "        print(\"dimensions: {}\".format(np.shape(W)))\n",
    "\n",
    "        axs[idx,0].imshow(W, # [:300,:300],\n",
    "                         aspect='auto')\n",
    "        #axs[0,idx].colorbar()\n",
    "        axs[idx,1].hist(np.tril(W,k=-1)[np.tril(W,k=-1).nonzero()].flatten(),\n",
    "                        histtype='step',\n",
    "                        bins=100)\n",
    "        axs[idx,1].set_xlim([0,1])\n",
    "        \n",
    "        print('--------------')\n",
    "\n",
    "        G_nx = nx.from_numpy_matrix(W)  # convert to graph for centrality computation\n",
    "        nx.set_node_attributes(G_nx,\n",
    "                               dict((i,w) for i,w in enumerate(model.wv.vocab)),\n",
    "                               'word')\n",
    "        d_eigcent = nx.eigenvector_centrality(G_nx, max_iter=200)\n",
    "        nx.set_node_attributes(G_nx, d_eigcent, 'eigenvector_centrality')\n",
    "        nx_graphs.append(G_nx)\n",
    "\n",
    "        samples[i_sample][model_name]['G_nx'] = G_nx\n",
    "        samples[i_sample][model_name]['P_thresh'] = P_thresh  # edge threshold on cosine scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the adjmats\n",
    "\n",
    "for i_sample, sample in enumerate(sample_IDs):\n",
    "    \n",
    "     # todo should we match vocabulary sizes?\n",
    "    clusters = samples[i_sample]['model_names']\n",
    "    (f, ax) = plt.subplots(len(clusters),\n",
    "                           3,\n",
    "                           figsize=(10,10))\n",
    "\n",
    "    community_results = []\n",
    "    for idx, model_name in enumerate(samples[i_sample]['short_names']):\n",
    "    #for idx, G in enumerate(nx_graphs):\n",
    "        G = samples[i_sample][model_name]['G_nx']\n",
    "\n",
    "        print('computing num triangles...')\n",
    "        #communities = nx.algorithms.community.label_propagation_communities(G)\n",
    "        #community_results.append([x for x in communities])\n",
    "\n",
    "        print('fetching adjmat...')\n",
    "        W = nx.convert_matrix.to_numpy_matrix(G)\n",
    "        print('computing cuthull mckee ordering...')\n",
    "        rcm = list(nx.utils.cuthill_mckee_ordering(G)) # pick the smallest to define the ordering\n",
    "        print('finished.')\n",
    "\n",
    "        W = nx.convert_matrix.to_numpy_matrix(G, nodelist=rcm)\n",
    "        ax[0,idx].imshow(W, aspect='auto', cmap='magma')\n",
    "        ax[0,idx].set_title('world {} in order #{}'.format(idx, idx))\n",
    "\n",
    "        degrees = [d for n,d in G.degree()]\n",
    "\n",
    "        #ax[0,idx].imshow(W,aspect='auto')\n",
    "        #ax[0,idx].set_title('world {}'.format(idx))\n",
    "\n",
    "        #ax[1,idx].set_ylim(bottom=0)\n",
    "        ax[1,idx].hist(degrees, bins=25, histtype='step')\n",
    "        ax[1,idx].set_title('degree histogram')\n",
    "        ax[1,idx].set_xlim([0,2500])\n",
    "\n",
    "        '''\n",
    "        d_triangles = nx.get_node_attributes(G,'triangles')\n",
    "        ax[2,idx].hist([count for count in d_triangles.values()], bins=25, histtype='step')\n",
    "        ax[2,idx].set_title('triangles histogram')\n",
    "        ax[2,idx].set_xlim([0,80000])\n",
    "        '''\n",
    "\n",
    "        d_eigcent = nx.get_node_attributes(G, 'eigenvector_centrality')\n",
    "        ax[2,idx].hist([count for count in d_eigcent.values()], bins=25, histtype='step')\n",
    "        ax[2,idx].set_title(' eigenvector centrality ')\n",
    "        ax[2,idx].set_xlim([0,0.04])\n",
    "        \n",
    "        samples[i_sample][model_name]['G_nx'] = G  # is this redundant?\n",
    "        \n",
    "        print('--------')\n",
    "\n",
    "    plt.savefig('figures_2/network_worlds_overview.pdf')\n",
    "    plt.savefig('figures_2/network_worlds_overview.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each adjmat in cuthill-mckee order (for the first sample)\n",
    "\n",
    "# todo orient the overview plot horizontally instead (mimicking the plot above)\n",
    "\n",
    "clusters = samples[0]['model_names']\n",
    "(f, ax) = plt.subplots(len(clusters),\n",
    "                       len(clusters),\n",
    "                       figsize=(10,10))\n",
    "\n",
    "for reference_id in range(len(clusters)):\n",
    "    \n",
    "    print('computing cuthull mckee ordering...')\n",
    "    rcm = list(nx.utils.cuthill_mckee_ordering(nx_graphs[reference_id])) # pick the smallest to define the ordering\n",
    "    print('finished.')\n",
    "\n",
    "    W = nx.convert_matrix.to_numpy_matrix(nx_graphs[reference_id], nodelist=rcm)\n",
    "    ax[reference_id,reference_id].imshow(W, aspect='auto', cmap='magma')\n",
    "    ax[reference_id,reference_id].set_title('world {} in order #{}'.format(reference_id, reference_id))\n",
    "\n",
    "#plt.savefig('cuthill_mckee.pdf')\n",
    "#plt.savefig('cuthill_mckee.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_eigcentral_nodes(sample_id, cluster_id, export_prefix, model_dir):\n",
    "    '''\n",
    "    for sample set specified by model_dir\n",
    "    for sample id := sample id\n",
    "    for cluster of pmids within the sample cluster_id\n",
    "    using name identifier := export prefix\n",
    "    \n",
    "    plot+save high eigenvector central nodes\n",
    "    plot+save the network of their interconnections\n",
    "    export the list of central words & corresponding node_ids\n",
    "    '''\n",
    "\n",
    "    # todo incorporate sample_id\n",
    "    model_name = samples[sample_id]['short_names'][cluster_id]\n",
    "    G_thresh = samples[sample_id][model_name]['G_nx']\n",
    "    \n",
    "    print('fetching node attributes...')\n",
    "    d_eigcent = nx.get_node_attributes(G_thresh,'eigenvector_centrality')\n",
    "    d_words = nx.get_node_attributes(G_thresh, 'word')\n",
    "\n",
    "    # sort ascending\n",
    "    top_nodes = [t[0] for t in sorted(d_eigcent.items(),\n",
    "                                      key=lambda x: x[1],\n",
    "                                      reverse=True)]\n",
    "    scores = [t[1] for t in sorted(d_eigcent.items(),\n",
    "                                      key=lambda x: x[1],\n",
    "                                      reverse=True)]\n",
    "\n",
    "    top_words = [d_words[i] for i in top_nodes]  # idxs -> words\n",
    "\n",
    "    # print\n",
    "    K = int(np.floor(0.015 * len(scores)))\n",
    "    print(top_nodes[:K])\n",
    "    print(top_words[:K])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(scores[:K],color='r')\n",
    "    plt.plot(scores, color=[0,0,0,0.25], linestyle=':')\n",
    "    \n",
    "    plt.savefig('figures_2/eigscores {} s{} c{}.png'.format(export_prefix,\n",
    "                                                            sample_id,\n",
    "                                                            cluster_id))\n",
    "    plt.savefig('figures_2/eigscores {} s{} c{}.pdf'.format(export_prefix,\n",
    "                                                            sample_id,\n",
    "                                                            cluster_id))\n",
    "\n",
    "    node_subset = top_nodes[:K]            \n",
    "    G = G_thresh.subgraph(node_subset)    \n",
    "    edges, weights = zip(*nx.get_edge_attributes(G, 'weight').items())\n",
    "\n",
    "    #print('fetching node attributes...')\n",
    "    #d_eigcent = nx.get_node_attributes(nx_graphs[cluster_id],'eigenvector_centrality')\n",
    "\n",
    "    print('generating labels...')\n",
    "    labels = dict( (node,top_words[i]) for i,node in enumerate(G.nodes()))\n",
    "    \n",
    "    print('computing layout...')\n",
    "    #d_layout_full = nx.spring_layout(nx_graphs[cluster_id]) # .subgraph(top_nodes[:2000]))  # TEMP cutoff for prototyping\n",
    "    #d_layout = {k: d_layout_full[k] for k in top_nodes[:K]}\n",
    "    #d_layout = {k: d_layout_full[k] for k in all_neighbors}\n",
    "    #d_layout = nx.random_layout(G)\n",
    "    d_layout = nx.spring_layout(G)\n",
    "    print('layout finished')\n",
    "\n",
    "    SCALE = 10000\n",
    "    node_sizes = SCALE*np.array([f for f in scores])\n",
    "    \n",
    "    print('plotting...')\n",
    "    plt.figure()\n",
    "    nx.draw(G,\n",
    "           pos=d_layout, # s[idx],  #d_layout\n",
    "           with_labels=False,\n",
    "           node_color=[[0.1,0.3,0.2,0.1]],\n",
    "           node_size=node_sizes, #  labels=labels,\n",
    "           edgelist=edges,\n",
    "           edge_color=weights,\n",
    "           width=0.25,\n",
    "           edge_cmap=plt.cm.Blues) \n",
    "    \n",
    "    plt.savefig('figures_2/layout {} s{} c{}.png'.format(export_prefix,\n",
    "                                                         sample_id,\n",
    "                                                         cluster_id))\n",
    "    plt.savefig('figures_2/layout {} s{} c{}.pdf'.format(export_prefix,\n",
    "                                                         sample_id,\n",
    "                                                         cluster_id))\n",
    "    save_obj = {'eigcentral_nodes': top_nodes,\n",
    "                'eigcentral_words': top_words}\n",
    "    save_path = \"{}/centrality s{} c{}.json\".format(model_dir,\n",
    "                                                     sample_id,\n",
    "                                                     cluster_id)\n",
    "    with open(save_path,'w') as f:\n",
    "        json.dump(save_obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version\n",
    "def plot_eigcent_nodes(cluster_id):\n",
    "    \n",
    "    d_eigcent = nx.get_node_attributes(nx_graphs[cluster_id],'eigenvector_centrality')\n",
    "    d_words = nx.get_node_attributes(nx_graphs[cluster_id], 'word')\n",
    "\n",
    "    # sort ascending\n",
    "    top_nodes = [t[0] for t in sorted(d_eigcent.items(),\n",
    "                                      key=lambda x: x[1],\n",
    "                                      reverse=True)]\n",
    "    scores = [t[1] for t in sorted(d_eigcent.items(),\n",
    "                                      key=lambda x: x[1],\n",
    "                                      reverse=True)]\n",
    "\n",
    "    top_words = [d_words[i] for i in top_nodes]  # idxs -> words\n",
    "\n",
    "    # print\n",
    "    K = int(np.floor(0.015 * len(scores)))\n",
    "    print(top_nodes[:K])\n",
    "    print(top_words[:K])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(scores[:K],color='r')\n",
    "    plt.plot(scores, color=[0,0,0,0.25], linestyle=':')\n",
    "    \n",
    "    plt.savefig('figures_2/eigscores {}.png'.format(cluster_id))\n",
    "    plt.savefig('figures_2/eigscores {}.pdf'.format(cluster_id))\n",
    "\n",
    "    node_subset = top_nodes[:K]\n",
    "    '''\n",
    "    print('crawling neighborhood...')\n",
    "    all_neighbors = []\n",
    "    for i,node in enumerate(node_subset):\n",
    "        all_neighbors.append(node)\n",
    "        for neighbor in nx_graphs[cluster_id][node]:\n",
    "            all_neighbors.append(neighbor)\n",
    "    all_neighbors = list(set(all_neighbors))\n",
    "    print('{} neighbors fetched'.format(len(all_neighbors)))\n",
    "    '''\n",
    "    all_neighbors = node_subset\n",
    "            \n",
    "    #G = nx_graphs[cluster_id].subgraph(node_subset)\n",
    "    G = nx_graphs[cluster_id].subgraph(all_neighbors)\n",
    "    \n",
    "    edges, weights = zip(*nx.get_edge_attributes(G, 'weight').items())\n",
    "    #print(weights)\n",
    "\n",
    "    print('fetching node attributes...')\n",
    "    d_eigcent = nx.get_node_attributes(nx_graphs[cluster_id],'eigenvector_centrality')\n",
    "\n",
    "    print('generating labels...')\n",
    "    labels = dict( (node,top_words[i]) for i,node in enumerate(G.nodes()))\n",
    "    \n",
    "    print('computing layout...')\n",
    "    d_layout_full = nx.spring_layout(nx_graphs[cluster_id]) # .subgraph(top_nodes[:2000]))  # TEMP cutoff for prototyping\n",
    "    #d_layout = {k: d_layout_full[k] for k in top_nodes[:K]}\n",
    "    d_layout = {k: d_layout_full[k] for k in all_neighbors}\n",
    "    #d_layout = nx.random_layout(G)\n",
    "    print('layout finished')\n",
    "\n",
    "    SCALE = 10000\n",
    "    node_sizes = SCALE*np.array([f for f in scores])\n",
    "    \n",
    "    print('plotting...')\n",
    "    plt.figure()\n",
    "    nx.draw(G,\n",
    "           pos=d_layout, # s[idx],  #d_layout\n",
    "           with_labels=False,\n",
    "           node_color=[[0.1,0.3,0.2,0.1]],\n",
    "           node_size=node_sizes, #  labels=labels,\n",
    "           edgelist=edges,\n",
    "           edge_color=weights,\n",
    "           width=0.25,\n",
    "           edge_cmap=plt.cm.Blues) \n",
    "    \n",
    "    plt.savefig('figures_2/layout {}.png'.format(cluster_id))\n",
    "    plt.savefig('figures_2/layout {}.pdf'.format(cluster_id))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_prefix = str.split(model_dir,'/')[-1] # grab the postfix\n",
    "\n",
    "for sample_id in range(N_samples):\n",
    "    print()\n",
    "    print('-------------------')\n",
    "    print('sample: {}'.format(sample_id))\n",
    "    N_clusters = len(samples[sample_id]['model_names'])\n",
    "    \n",
    "    for cluster_id in range(N_clusters):\n",
    "        print()\n",
    "        print('sample {} cluster {}'.format(sample_id, cluster_id))\n",
    "        id_eigcentral_nodes(sample_id, cluster_id, export_prefix, model_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 0\n",
    "plot_eigcent_nodes(cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 1\n",
    "plot_eigcent_nodes(cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 2\n",
    "plot_eigcent_nodes(cluster_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
