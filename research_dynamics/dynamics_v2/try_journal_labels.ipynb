{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['pdf.fonttype'] = '42'\n",
    "\n",
    "import time\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  use journal names to help interpret the space of papers\n",
    "\n",
    "# todo also try using titles (they may be braoder scale labels than abstract words)\n",
    "                          #  ( but finer scale and more plentiful than journal names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configure & initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## control params\n",
    "\n",
    "year = 2018   # pmids are listed by year in preprocessed json files\n",
    "path2dir = '/home/brendan/FastData/pubmed2019/pubmed_data_processing/year_pmids/'  # knowledge-garden\n",
    "      #  = '/project2/jevans/brendan/pubmed_data_processing/year_pmids/'  # RCC Midway2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysql credentials\n",
    "config_path = '/home/brendan/Projects/AttentionWildfires/attention_wildfires/mysql_config.json'\n",
    "db_name = 'test_pubmed'  # db name collison issue? https://stackoverflow.com/questions/14011968/user-cant-access-a-database\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config_data = json.load(f)\n",
    "    \n",
    "client_config = {'database': db_name,\n",
    "                'user': config_data['user'],\n",
    "                 'password': config_data['lock']}\n",
    "\n",
    "db = pymysql.connect(**client_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop open pickle jar: umap model...\n"
     ]
    }
   ],
   "source": [
    "# dimensionality reduction\n",
    "\n",
    "########################\n",
    "## load pre-fit pca models (fit using samples (size=100k) from the full corpus)\n",
    "\n",
    "#pca_path = '/project2/jevans/brendan/pubmed_data_processing/dimensionality_reduction_models/pca_models/pca_model0.pkl'  # more located at /project2...\n",
    "#with open(pca_path, 'rb') as file:\n",
    "#    pca_model = pickle.load(file)\n",
    "    \n",
    "#########################\n",
    "\n",
    "# load the pre-fit umap model  (trained on time-flattened data)\n",
    "\n",
    "print('pop open pickle jar: umap model...')\n",
    "umap_path = \"/home/brendan/FastData/pubmed2019/pubmed_data_processing/dimensionality_reduction_models/umap2D/umap_model0.pkl\"\n",
    "with open(umap_path, 'rb') as file:\n",
    "    umap_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fetch samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N pubs: 1205220\n"
     ]
    }
   ],
   "source": [
    "# load list of pmids published in this year\n",
    "\n",
    "filename = 'pubmed_state_{}'.format(year)\n",
    "path2pmids = path2dir + filename\n",
    "with open(path2pmids,'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "year_pub_pmids = data['publications']\n",
    "N_pubs = len(year_pub_pmids)\n",
    "print(\"N pubs: {}\".format(N_pubs))\n",
    "del data # clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample indices\n",
    "\n",
    "K_sample = 10000\n",
    "N_samplesets = 2\n",
    "samples = {}\n",
    "for i in range(N_samplesets):\n",
    "    samples[i] = {}\n",
    "    samples[i]['pmids'] = np.random.choice(year_pub_pmids, K_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for selecting the embedding vectors\n",
    "\n",
    "def get_embedding_vectors(sample_pmids):\n",
    "    str_fmt = ', '.join([str(pmid) for pmid in sample_pmids])\n",
    "\n",
    "    sql = '''SELECT E.pmid, E.embedding\n",
    "            FROM scibert_mean_embedding as E\n",
    "            WHERE E.pmid IN ({})'''.format(str_fmt)\n",
    "\n",
    "    start_time = time.time()\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(sql)\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(\"SQL join executed in {} s\".format(elapsed))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pub_embeddings = []\n",
    "    pub_pmids = []\n",
    "    for i,row in enumerate(cursor):\n",
    "        print_block_len = 100000\n",
    "        if (i+1) % print_block_len == 0:\n",
    "            print('fetched {} rows...'.format(print_block_len))\n",
    "        pub_pmids.append(row[0])\n",
    "        pub_embeddings.append(np.frombuffer(row[1],dtype='float16').tolist())\n",
    "    cursor.close()\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(\"SQL results fetched and cast in {} s\".format(elapsed))\n",
    "    \n",
    "    return pub_pmids, pub_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL join executed in 0.7758200168609619 s\n",
      "SQL results fetched and cast in 0.18572497367858887 s\n",
      "compressing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendan/anaconda3/envs/embedding-feb2020/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../brendanchambers/.conda/envs/embedding-base/lib/python3.7/site-packages/umap/nndescent.py\", line 123:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/home/brendan/anaconda3/envs/embedding-feb2020/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../brendanchambers/.conda/envs/embedding-base/lib/python3.7/site-packages/umap/nndescent.py\", line 134:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    }
   ],
   "source": [
    "# populate embedding vectors for these samples\n",
    "\n",
    "for i in range(N_samplesets):\n",
    "    pmids, embeddings = get_embedding_vectors(samples[i]['pmids'])\n",
    "    samples[i]['pmids'] = pmids  # remove pmids with no corresponding embedding (e.g. no abstract)\n",
    "    samples[i]['raw_coordinates'] = embeddings\n",
    "    print('compressing...')\n",
    "    samples[i]['umap2D'] = umap_model.transform(embeddings)\n",
    "    \n",
    "# note on the warning-\n",
    "#   this seems to be a known issue related to tuples in numba no-python mode\n",
    "#    it's arising due to umap internals + the numba bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check - compare umap plots of the two samples\n",
    "\n",
    "(f, ax) = plt.subplots(1,\n",
    "                       2,\n",
    "                       sharex='all', sharey='all',\n",
    "                       figsize=(10,4))\n",
    "\n",
    "sns.kdeplot(samples[0]['umap2D'][:,0], # these are pca'd\n",
    "            samples[0]['umap2D'][:,1],\n",
    "            ax=ax[0],\n",
    "            shade=True,\n",
    "    shade_lowest=False,\n",
    "            cmap='Blues')\n",
    "ax[0].set_title('published: year {}'.format(year))\n",
    "\n",
    "sns.kdeplot(samples[1]['umap2D'][:,0], # these are pca'd\n",
    "            samples[1]['umap2D'][:,1],\n",
    "            ax=ax[1],\n",
    "            shade=True,\n",
    "    shade_lowest=False,\n",
    "            cmap='Blues')\n",
    "ax[0].set_title('published: year {}'.format(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin experimental exploration of journals\n",
    "#   # should also build embedding space using the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check top 12 journals for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_sample in range(N_samplesets):\n",
    "    \n",
    "    sample_pmids = samples[i_sample]['pmids']\n",
    "    \n",
    "    str_fmt = ', '.join([str(pmid) for pmid in sample_pmids])\n",
    "\n",
    "    sql = '''SELECT M.journal, COUNT(*) as count\n",
    "            FROM metadata as M\n",
    "            WHERE M.pmid IN ({})\n",
    "            GROUP BY M.journal\n",
    "            ORDER BY count DESC'''.format(str_fmt)\n",
    "\n",
    "    start_time = time.time()\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(sql)\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(\"SQL join executed in {} s\".format(elapsed))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    top_journals = []\n",
    "    journal_counts = []\n",
    "    for i,row in enumerate(cursor):\n",
    "        print_block_len = 100000\n",
    "        if (i+1) % print_block_len == 0:\n",
    "            print('fetched {} rows...'.format(print_block_len))\n",
    "        top_journals.append(row[0])\n",
    "        journal_counts.append(row[1])\n",
    "    cursor.close()\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(\"SQL results fetched and cast in {} s\".format(elapsed))\n",
    "    \n",
    "    print('number of unique journals in the sample: ')\n",
    "    print(len(top_journals)) # sanity check\n",
    "    print(len(journal_counts))\n",
    "    sample_size = np.sum(journal_counts)\n",
    "    journal_counts = journal_counts / sample_size\n",
    "    \n",
    "    topK = 12\n",
    "    plt.figure(figsize=(3,10))\n",
    "    plt.gca().set_yscale('log')\n",
    "    plt.plot(journal_counts[:topK])\n",
    "    for i_journal, journal in enumerate(top_journals[:topK]):\n",
    "        plt.scatter(i_journal, journal_counts[i_journal],marker='o')\n",
    "        plt.text(0, journal_counts[i_journal], journal, alpha=0.5)\n",
    "        \n",
    "    plt.savefig(\"sample {}: top journals.png\".format(i_sample))\n",
    "    plt.savefig(\"sample {}: top journals.pdf\".format(i_sample))\n",
    "    print(top_journals[:topK])\n",
    "    \n",
    "    ######################################################################\n",
    "    \n",
    "    sample_pmids = samples[i_sample]['pmids']\n",
    "    str_fmt = ', '.join([str(pmid) for pmid in sample_pmids])\n",
    "    \n",
    "    sql = '''SELECT M.pmid, M.journal\n",
    "            FROM metadata as M\n",
    "            WHERE M.pmid IN ({})\n",
    "            ORDER BY M.journal'''.format(str_fmt,\n",
    "                                        top_journals_str)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(sql)\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "  # print(\"SQL join executed in {} s\".format(elapsed))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = {}\n",
    "    for i,row in enumerate(cursor):\n",
    "        #print(row)\n",
    "        print_block_len = 100000\n",
    "        if (i+1) % print_block_len == 0:\n",
    "            print('fetched {} rows...'.format(print_block_len))\n",
    "        try:\n",
    "            results[row[1]].append(row[0])\n",
    "        except:\n",
    "            results[row[1]] = []\n",
    "            results[row[1]].append(row[0])\n",
    "    cursor.close()\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    #print(\"SQL results fetched and cast in {} s\".format(elapsed))\n",
    "    \n",
    "    # filter results so they include only the top K journals (don't really need this step)\n",
    "    filtered_results = {}\n",
    "    for journal,pmids in results.items():\n",
    "        if journal in top_journals[:topK]:\n",
    "            filtered_results[journal] = pmids\n",
    "            \n",
    "            \n",
    "    # plot the journal coordinates for the top K journals\n",
    "    N_COL, N_ROW = 3, 4\n",
    "    fig, axs = plt.subplots(N_COL, N_ROW, sharex='col', sharey='row',\n",
    "                           figsize=(16,12))\n",
    "    \n",
    "    for i_journal, journal in enumerate(top_journals[:topK]):\n",
    "        \n",
    "        # subplot grid\n",
    "        i_col = math.floor(i_journal / N_COL)\n",
    "        i_row = i_journal % N_COL\n",
    "        \n",
    "        journal_pmids = filtered_results[journal]\n",
    "        str_fmt = ', '.join([str(pmid) for pmid in journal_pmids])\n",
    "\n",
    "        sql = '''SELECT E.pmid, E.embedding\n",
    "                FROM scibert_mean_embedding as E\n",
    "                WHERE E.pmid IN ({})'''.format(str_fmt)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        cursor = db.cursor()\n",
    "        cursor.execute(sql)\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        #print(\"SQL join executed in {} s\".format(elapsed))\n",
    "\n",
    "        start_time = time.time()\n",
    "        pub_embeddings = []\n",
    "        pub_pmids = []\n",
    "        for i,row in enumerate(cursor):\n",
    "            print_block_len = 100000\n",
    "            if (i+1) % print_block_len == 0:\n",
    "                print('fetched {} rows...'.format(print_block_len))\n",
    "            pub_pmids.append(row[0])\n",
    "            pub_embeddings.append(np.frombuffer(row[1],dtype='float16').tolist())\n",
    "        cursor.close()\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        #print(\"SQL results fetched and cast in {} s\".format(elapsed))\n",
    "        \n",
    "        #print('compressing...')\n",
    "        journal_coords = umap_model.transform(pub_embeddings)\n",
    "        #print('finished compressing')\n",
    "        \n",
    "        axs[i_row][i_col].scatter(journal_coords[:,0], journal_coords[:,1], marker='.')\n",
    "        axs[i_row][i_col].set_title('journal: {}'.format(journal))\n",
    "    \n",
    "        plt.savefig('top 12 journals in sample.png')\n",
    "        plt.savefig('top 12 journals in sample.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 random journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check top 12 journals for each sample\n",
    "\n",
    "for i_sample in range(N_samplesets):\n",
    "    \n",
    "    sample_pmids = samples[i_sample]['pmids']\n",
    "    \n",
    "    str_fmt = ', '.join([str(pmid) for pmid in sample_pmids])\n",
    "\n",
    "    sql = '''SELECT M.journal, COUNT(*) as count\n",
    "            FROM metadata as M\n",
    "            WHERE M.pmid IN ({})\n",
    "            GROUP BY M.journal\n",
    "            ORDER BY RAND()'''.format(str_fmt)\n",
    "\n",
    "    start_time = time.time()\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(sql)\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(\"SQL join executed in {} s\".format(elapsed))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    top_journals = []\n",
    "    journal_counts = []\n",
    "    for i,row in enumerate(cursor):\n",
    "        print_block_len = 100000\n",
    "        if (i+1) % print_block_len == 0:\n",
    "            print('fetched {} rows...'.format(print_block_len))\n",
    "        top_journals.append(row[0])\n",
    "        journal_counts.append(row[1])\n",
    "    cursor.close()\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(\"SQL results fetched and cast in {} s\".format(elapsed))\n",
    "    \n",
    "    print(len(top_journals)) # sanity check\n",
    "    print(len(journal_counts))\n",
    "    sample_size = np.sum(journal_counts)\n",
    "    journal_counts = journal_counts / sample_size\n",
    "    \n",
    "    topK = 12\n",
    "    plt.figure(figsize=(3,10))\n",
    "    plt.gca().set_yscale('log')\n",
    "    plt.plot(journal_counts[:topK])\n",
    "    for i_journal, journal in enumerate(top_journals[:topK]):\n",
    "        plt.scatter(i_journal, journal_counts[i_journal],marker='o')\n",
    "        plt.text(0, journal_counts[i_journal], journal, alpha=0.5)\n",
    "        \n",
    "    plt.savefig(\"sample {}: top journals.png\".format(i_sample))\n",
    "    plt.savefig(\"sample {}: top journals.pdf\".format(i_sample))\n",
    "    print(top_journals[:topK])\n",
    "    \n",
    "    ######################################################################\n",
    "    \n",
    "    sample_pmids = samples[i_sample]['pmids']\n",
    "    str_fmt = ', '.join([str(pmid) for pmid in sample_pmids])\n",
    "    \n",
    "    sql = '''SELECT M.pmid, M.journal\n",
    "            FROM metadata as M\n",
    "            WHERE M.pmid IN ({})\n",
    "            ORDER BY M.journal'''.format(str_fmt,\n",
    "                                        top_journals_str)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(sql)\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "  # print(\"SQL join executed in {} s\".format(elapsed))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = {}\n",
    "    for i,row in enumerate(cursor):\n",
    "        #print(row)\n",
    "        print_block_len = 100000\n",
    "        if (i+1) % print_block_len == 0:\n",
    "            print('fetched {} rows...'.format(print_block_len))\n",
    "        try:\n",
    "            results[row[1]].append(row[0])\n",
    "        except:\n",
    "            results[row[1]] = []\n",
    "            results[row[1]].append(row[0])\n",
    "    cursor.close()\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    #print(\"SQL results fetched and cast in {} s\".format(elapsed))\n",
    "    \n",
    "    # filter results so they include only the top K journals (don't really need this step)\n",
    "    filtered_results = {}\n",
    "    for journal,pmids in results.items():\n",
    "        if journal in top_journals[:topK]:\n",
    "            filtered_results[journal] = pmids\n",
    "            \n",
    "            \n",
    "    # plot the journal coordinates for the top K journals\n",
    "    N_COL, N_ROW = 3, 4\n",
    "    fig, axs = plt.subplots(N_COL, N_ROW, sharex='col', sharey='row',\n",
    "                           figsize=(16,12))\n",
    "    \n",
    "    for i_journal, journal in enumerate(top_journals[:topK]):\n",
    "        \n",
    "        # subplot grid\n",
    "        i_col = math.floor(i_journal / N_COL)\n",
    "        i_row = i_journal % N_COL\n",
    "        \n",
    "        journal_pmids = filtered_results[journal]\n",
    "        str_fmt = ', '.join([str(pmid) for pmid in journal_pmids])\n",
    "\n",
    "        sql = '''SELECT E.pmid, E.embedding\n",
    "                FROM scibert_mean_embedding as E\n",
    "                WHERE E.pmid IN ({})'''.format(str_fmt)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        cursor = db.cursor()\n",
    "        cursor.execute(sql)\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        #print(\"SQL join executed in {} s\".format(elapsed))\n",
    "\n",
    "        start_time = time.time()\n",
    "        pub_embeddings = []\n",
    "        pub_pmids = []\n",
    "        for i,row in enumerate(cursor):\n",
    "            print_block_len = 100000\n",
    "            if (i+1) % print_block_len == 0:\n",
    "                print('fetched {} rows...'.format(print_block_len))\n",
    "            pub_pmids.append(row[0])\n",
    "            pub_embeddings.append(np.frombuffer(row[1],dtype='float16').tolist())\n",
    "        cursor.close()\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        #print(\"SQL results fetched and cast in {} s\".format(elapsed))\n",
    "        \n",
    "        #print('compressing...')\n",
    "        journal_coords = umap_model.transform(pub_embeddings)\n",
    "        #print('finished compressing')\n",
    "        \n",
    "        axs[i_row][i_col].scatter(journal_coords[:,0], journal_coords[:,1], marker='.')\n",
    "        axs[i_row][i_col].set_title('journal: {}'.format(journal))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of journals for 10 random journals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
