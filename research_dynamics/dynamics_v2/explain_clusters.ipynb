{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate lists of words to explain the clusters\n",
    "\n",
    "### Jan 9 2020, Brendan Chambers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "import pymysql\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import networkx as nx\n",
    "\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from pmids2vec import pmids2vec\n",
    "from pmids2corpus import pmids2corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in word2vec models trained on each coarse cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp, recreate the list of file names without re-training the w2v models\n",
    "\n",
    "model_names = ['data_processing/cluster0.model',\n",
    "        'data_processing/cluster1.model',\n",
    "        'data_processing/cluster2.model']\n",
    "print(model_names)\n",
    "print()\n",
    "\n",
    "models = {}\n",
    "for model_name in model_names:\n",
    "    \n",
    "    short_name = str.split(model_name,'/')[-1]\n",
    "    print(short_name)\n",
    "    \n",
    "    model = Word2Vec.load(model_name)\n",
    "    models[short_name] = model\n",
    "    print('{} words '.format(len(model.wv.vocab)))\n",
    "    print()\n",
    "    \n",
    "#  ~16,000 words across all 3 clusters (> 25 times)\n",
    "#  ~3,000 words occurring in all 3 clusters (> 25 times)\n",
    "#  window size 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a networkx graph for each word similarity network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo use unique vocabularies for starters\n",
    "\n",
    "# todo match vocabulary sizes\n",
    "\n",
    "\n",
    "nx_graphs = []\n",
    "for idx, model in enumerate(models.values()):\n",
    "    \n",
    "    # the number of nodes is small so we can do this with mat mul\n",
    "    D = np.shape(model.wv[common_vocabulary[0]])[0]\n",
    "    \n",
    "    embedding_samples = np.zeros( (len(model.wv.vocab), D) )\n",
    "    \n",
    "    # collect samples\n",
    "    for i_word, word in enumerate(model.wv.vocab):\n",
    "        embedding_samples[i_word,:] = model.wv[word]\n",
    "        \n",
    "    # reduce dimensionality\n",
    "    #D_umap = 5\n",
    "    #reducer = umap.UMAP(n_components=D_umap)\n",
    "    #um = reducer.fit_transform(embedding_samples)    # concatenated\n",
    "    #embedding_samples = um\n",
    "    \n",
    "    # demean\n",
    "    for i_word, word in enumerate(model.wv.vocab):\n",
    "        embedding_samples[i_word,:] -= np.mean(embedding_samples,0)\n",
    "    \n",
    "    # normalize all vectors to the hypersphere\n",
    "    for i_row,row in enumerate(embedding_samples):\n",
    "        embedding_samples[i_row,:] /= np.linalg.norm(row,2)\n",
    "        \n",
    "    # compute cosine similarity (projection of normalized vectors)\n",
    "    W = np.matmul(embedding_samples, embedding_samples.T)\n",
    "    np.fill_diagonal(W, 0)  # no self-loops\n",
    "    thresh_low = np.percentile(W.flatten(), 1)\n",
    "    thresh_high = np.percentile(W.flatten(), 98)\n",
    "    #boolean_mask = np.logical_and(W < thresh_high, W > thresh_low)\n",
    "    boolean_mask = W < thresh_high\n",
    "    W[boolean_mask] = 0 # mask out some weights for testing\n",
    "    print(np.shape(W))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(W)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(np.tril(W,k=-1)[np.tril(W,k=-1).nonzero()].flatten(),histtype='step', bins=100)\n",
    "    \n",
    "    #W_binary = (W > 0).tolist()\n",
    "    #ggg[idx] = igraph.Graph.Adjacency(W_binary)  # define connections\n",
    "    #ggg[idx].es['weight'] = W[W.nonzero()]\n",
    "    #G_igraph = igraph.Graph.Weighted_Adjacency(W.tolist(),\n",
    "    #                                           mode=igraph.ADJ_UNDIRECTED,\n",
    "    #                                          loops=False,\n",
    "    #                                          attr='weight')\n",
    "    #igraphs.append(G_igraph)\n",
    "    \n",
    "    # create networkx version\n",
    "    G_nx = nx.from_numpy_matrix(W)  # create using - \n",
    "    nx.set_node_attributes(G_nx,\n",
    "                           dict((i,w) for i,w in enumerate(model.wv.vocab)),\n",
    "                           'word')\n",
    "    d_eigcent = nx.eigenvector_centrality(G_nx, max_iter=200)\n",
    "    nx.set_node_attributes(G_nx, d_eigcent, 'eigenvector_centrality')\n",
    "    nx_graphs.append(G_nx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
