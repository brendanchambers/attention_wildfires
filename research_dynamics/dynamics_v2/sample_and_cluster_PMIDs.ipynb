{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  sample and cluster PMIDs from a single year\n",
    "###  export PMIDs grouped by cluster, with umap coordinates for convenience\n",
    "\n",
    "todo on the next pass, use ~5D umap coords and multiple samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "import time\n",
    "import hdbscan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file = 'data_processing_feb2020/pmids_2018_250k.json'  # name to assign exported results\n",
    "\n",
    "K_sample = 250000   # rule of thumb - at least 5K samples per cluster for abstracts analysis\n",
    "                   #                 - at least 25K samples per cluster for titles analysis\n",
    "\n",
    "N_samplesets = 3\n",
    "year = 2018\n",
    "path2dir = '/home/brendan/FastData/pubmed2019/pubmed_data_processing/year_pmids/'  # knowledge-garden\n",
    "        #path2dir = '/project2/jevans/brendan/pubmed_data_processing/year_pmids/'  # RCC Midway2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up mysql connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '/home/brendan/Projects/AttentionWildfires/attention_wildfires/mysql_config.json'\n",
    "db_name = 'test_pubmed'  # db name collisons? https://stackoverflow.com/questions/14011968/user-cant-access-a-database\n",
    "                        # todo should move this db_name into config file\n",
    "with open(config_path, 'r') as f:\n",
    "    config_data = json.load(f)\n",
    "    \n",
    "client_config = {'database': db_name,\n",
    "                'user': config_data['user'],\n",
    "                 'password': config_data['lock']}\n",
    "\n",
    "## init db connection\n",
    "db = pymysql.connect(**client_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the pre-fit umap model (trained on time-flattened data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop open pickle jar: umap model...\n"
     ]
    }
   ],
   "source": [
    "print('pop open pickle jar: umap model...')\n",
    "umap_path = \"/home/brendan/FastData/pubmed2019/pubmed_data_processing/dimensionality_reduction_models/umap2D/umap_model0.pkl\"\n",
    "with open(umap_path, 'rb') as file:\n",
    "    umap_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the precomputed set of pmids for this year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N pubs: 1205220\n"
     ]
    }
   ],
   "source": [
    "filename = 'pubmed_state_{}'.format(year)\n",
    "path2pmids = path2dir + filename\n",
    "with open(path2pmids,'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "year_pub_pmids = data['publications']\n",
    "N_pubs = len(year_pub_pmids)\n",
    "print(\"N pubs: {}\".format(N_pubs))\n",
    "del data # clean up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate sample indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "for i in range(N_samplesets):\n",
    "    samples[i] = {}\n",
    "    samples[i]['pmids'] = np.random.choice(year_pub_pmids, K_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper function for selecting embedding coordinates (for density based clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vectors(sample_pmids):\n",
    "    start_time = time.time()\n",
    "        \n",
    "    str_fmt = ', '.join([str(pmid) for pmid in sample_pmids])\n",
    "    sql = '''SELECT E.pmid, E.embedding\n",
    "            FROM scibert_mean_embedding as E\n",
    "            WHERE E.pmid IN ({})'''.format(str_fmt)\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "    pub_embeddings = []\n",
    "    pub_pmids = []\n",
    "    for i,row in enumerate(cursor):\n",
    "        pub_pmids.append(row[0])\n",
    "        pub_embeddings.append(np.frombuffer(row[1],dtype='float16').tolist())\n",
    "    cursor.close()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(\"SQL query composed and executed in {} s\".format(elapsed))\n",
    "    \n",
    "    return pub_pmids, pub_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use sample indices to fetch embedding coordinates & compress with umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendan/anaconda3/envs/embedding-feb2020/lib/python3.7/site-packages/pymysql/cursors.py:170: Warning: (3170, \"Memory capacity of 8388608 bytes for 'range_optimizer_max_mem_size' exceeded. Range optimization was not done for this query.\")\n",
      "  result = self._query(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL query composed and executed in 48.01088619232178 s\n",
      "compressing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendan/anaconda3/envs/embedding-feb2020/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../brendanchambers/.conda/envs/embedding-base/lib/python3.7/site-packages/umap/nndescent.py\", line 123:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/home/brendan/anaconda3/envs/embedding-feb2020/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../../../../../brendanchambers/.conda/envs/embedding-base/lib/python3.7/site-packages/umap/nndescent.py\", line 134:\n",
      "<source missing, REPL/exec in use?>\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    }
   ],
   "source": [
    "for i in range(N_samplesets):\n",
    "    pmids, embeddings = get_embedding_vectors(samples[i]['pmids'])\n",
    "    samples[i]['pmids'] = pmids  # remove pmids with no corresponding embedding (e.g. no abstract)\n",
    "    samples[i]['raw_coordinates'] = embeddings\n",
    "    print('compressing...')\n",
    "    samples[i]['umap2D'] = umap_model.transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity check - compare umap plots of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    (f, ax) = plt.subplots(1,\n",
    "                           N_samplesets,\n",
    "                           sharex='all', sharey='all',\n",
    "                           figsize=(12,4))\n",
    "\n",
    "    for i_sample in range(N_samplesets):\n",
    "\n",
    "        sns.kdeplot(samples[i_sample]['umap2D'][:,0], # these are pca'd\n",
    "                    samples[i_sample]['umap2D'][:,1],\n",
    "                    ax=ax[i_sample],\n",
    "                    shade=True,\n",
    "                    shade_lowest=False,\n",
    "                    cmap='Blues')\n",
    "        ax[i_sample].set_title('pubs: sample {} year {}'.format(i_sample, year))\n",
    "except:\n",
    "    # subplot grid will fail if there is only one sample\n",
    "    \n",
    "    plt.figure()\n",
    "    i_sample = 0\n",
    "    sns.kdeplot(samples[i_sample]['umap2D'][:,0], # these are pca'd\n",
    "                        samples[i_sample]['umap2D'][:,1],\n",
    "                        shade=True,\n",
    "                        shade_lowest=False,\n",
    "                        cmap='Blues')\n",
    "    plt.title('pubs: sample {} year {}'.format(i_sample, year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cluster with hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare clustering results across the two samples\n",
    "clustering_scale = int(K_sample * 0.02)\n",
    "min_samples_param = int(np.min([1000, clustering_scale]))\n",
    "\n",
    "clusterers = {}\n",
    "for i in range(N_samplesets):\n",
    "    clusterers[i] = hdbscan.HDBSCAN(min_cluster_size=clustering_scale, # 500 for 25K # 1000 for 50K # 50 fro 2000\n",
    "                            min_samples=min_samples_param,   # 500, 1000, 50\n",
    "                            cluster_selection_method='leaf')  # euclidean distance\n",
    "    clusterers[i].fit(samples[i]['umap2D'])  # samples x features\n",
    "\n",
    "    # number of clusters\n",
    "    print('num clusters: {}'.format(clusterers[i].labels_.max()+1))\n",
    "    \n",
    "    # plot clusters\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.scatter(samples[i]['umap2D'][:,0],\n",
    "               samples[i]['umap2D'][:,1],\n",
    "               c=clusterers[i].labels_)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # plot clusters using probabilities_ estimate  (todo combine plots)\n",
    "    rgba = np.zeros( (np.shape(samples[i]['umap2D'])[0], 4) )\n",
    "    rgba[:,3] = clusterers[i].probabilities_\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.scatter(samples[i]['umap2D'][:,0],\n",
    "               samples[i]['umap2D'][:,1],\n",
    "               c=rgba)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.savefig('raw clustering {}.png'.format(i))\n",
    "    plt.savefig('raw clustering {}.pdf'.format(i))\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize labels (for easier aggregation in postprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign label ID using appx clockwise angle, break ties with radius\n",
    "\n",
    "R_MAX = 10\n",
    "def serialize_point(x,y):\n",
    "    theta = math.atan2(x,y)\n",
    "    theta = theta / math.pi # rescale onto (-1,1)\n",
    "    if theta < 0:  # restitch 0-angle\n",
    "        theta = theta + 2\n",
    "    theta = round(theta,1) # limit precision of theta and break ties with r\n",
    "    r = math.sqrt(x*x + y*y)\n",
    "    r = r / R_MAX  # rescale onto (0,1)\n",
    "    score = 10*theta + r  # use angle first, break ties with radius\n",
    "    return score\n",
    "\n",
    "# sanity testing\n",
    "'''\n",
    "print(serialize_point(0,0))\n",
    "print(serialize_point(0,1))\n",
    "print(serialize_point(1,1))\n",
    "print(serialize_point(1,0))\n",
    "print(serialize_point(-1,-1))\n",
    "print(serialize_point(-1,0))\n",
    "plt.scatter([0,0,1,1,-1,-1],[0,1,1,0,-1,0])\n",
    "'''\n",
    "\n",
    "for i in range(N_samplesets):\n",
    "\n",
    "    # number of clusters\n",
    "    N_clusters = int(clusterers[i].labels_.max()+1)\n",
    "    print('num clusters: {}'.format(N_clusters))\n",
    "    \n",
    "    # get summary coordinate for each cluster\n",
    "    #cluster_mean = np.zeros( (N_clusters, 2))  # x,y (would be better to use median though)\n",
    "    cluster_median = np.zeros( (N_clusters, 2))  # x,y (would be better to use median though)\n",
    "    for i_cluster in range(N_clusters):\n",
    "        total_points = 0\n",
    "        vector_sum = np.zeros((1,2))\n",
    "        \n",
    "        convenience_xx, convenience_yy = [], []\n",
    "        for i_label, label in enumerate(clusterers[i].labels_):\n",
    "            # do nothing for labels of -1\n",
    "            if label==i_cluster:\n",
    "                convenience_xx.append(samples[i]['umap2D'][i_label,0])\n",
    "                convenience_yy.append(samples[i]['umap2D'][i_label,1])\n",
    "                \n",
    "        median_x = np.median(convenience_xx)\n",
    "        median_y = np.median(convenience_yy)\n",
    "        cluster_median[i_cluster,:] = (median_x, median_y)                                \n",
    "                \n",
    "                #total_points += 1\n",
    "                #vector_sum += [samples[i]['umap2D'][i_label,0],\n",
    "                #               samples[i]['umap2D'][i_label,1]]  # if this is higher D just use first two dimensions\n",
    "        #cluster_mean[i_cluster,:] = 1.0*vector_sum / total_points\n",
    "    \n",
    "    # sanity check\n",
    "    #plt.figure()\n",
    "    #plt.scatter(cluster_mean[:,0],cluster_mean[:,1],marker='x')\n",
    "    \n",
    "    # use summary coordinate to order the clusters based on a serialization function\n",
    "    cluster_scores = []\n",
    "    for i_cluster in range(N_clusters):\n",
    "        #score = serialize_point(cluster_mean[i_cluster,0],  # apply custom ordering function\n",
    "        #                        cluster_mean[i_cluster,1])\n",
    "        score = serialize_point(cluster_median[i_cluster,0],\n",
    "                               cluster_median[i_cluster,1])\n",
    "        cluster_scores.append(score)\n",
    "    # sort ascending\n",
    "    print(cluster_scores)\n",
    "    new_labels = np.argsort(np.argsort(cluster_scores))\n",
    "    print(new_labels)\n",
    "    \n",
    "    # re-assign labels after normalizing integer names for consistency\n",
    "    num_samples = len(clusterers[i].labels_)\n",
    "    newlabels_ = np.zeros( (num_samples,) ).astype(int)\n",
    "    for i_label, old_label in enumerate(clusterers[i].labels_):\n",
    "        if old_label==-1:\n",
    "            newlabels_[i_label] = -1  # populate new labels\n",
    "        else:\n",
    "            newlabels_[i_label] = new_labels[int(old_label)]  # cast as integer (temp hack)\n",
    "    for i_label, old_label in enumerate(clusterers[i].labels_):  # rewrite with new labels\n",
    "        clusterers[i].labels_[i_label] = newlabels_[i_label]\n",
    "        \n",
    "    # plot clusters\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.scatter(samples[i]['umap2D'][:,0],\n",
    "               samples[i]['umap2D'][:,1],\n",
    "               c=newlabels_) #clusterers[i].labels_)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.savefig('serialized clustering {}.png'.format(i))\n",
    "    plt.savefig('serialized clustering {}.pdf'.format(i))\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  format for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pmids = {}  # for export\n",
    "summary_coords = {}\n",
    "for i_sample in range(N_samplesets):\n",
    "    print(\"sample {}\".format(i_sample))\n",
    "    \n",
    "    pmids[i_sample] = {}   # for export\n",
    "    summary_coords[i_sample] = {}\n",
    "    \n",
    "    sample_pmids = np.array(samples[i_sample]['pmids'])  # for convenience, cast list -> np array\n",
    "    sample_coords = np.array(samples[i_sample]['umap2D'])\n",
    "    \n",
    "    k_clusters = int(clusterers[i_sample].labels_.max() + 1)\n",
    "    for i_cluster in range(k_clusters):\n",
    "              \n",
    "        cluster_pmids = sample_pmids[clusterers[i_sample].labels_ == i_cluster]    \n",
    "        cluster_coords = sample_coords[clusterers[i_sample].labels_ == i_cluster]\n",
    "        print(np.shape(cluster_coords))\n",
    "        \n",
    "        pmids[i_sample][i_cluster] = cluster_pmids.tolist()   # format for export\n",
    "        summary_coords[i_sample][i_cluster] = cluster_coords.tolist()\n",
    "    print('-------------')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj = {'pmids': pmids,\n",
    "            'summary_coords': summary_coords}\n",
    "\n",
    "with open(target_file,'w') as f:\n",
    "    json.dump(save_obj, f, indent=2, sort_keys=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
