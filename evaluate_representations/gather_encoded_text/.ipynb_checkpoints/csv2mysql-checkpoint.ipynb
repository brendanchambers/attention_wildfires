{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Representations pipeline: 'Gather'\n",
    "            consolidate encoding outputs (csv files) to mysql tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import mysql.connector as mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pure mysql python connector approach to the mysql connection\n",
    "#   offers easy control over the BLOB type embeddings\n",
    "client_config = {'unix_socket':'/home/brendanchambers/.sql.sock',\n",
    "                            'database':'test_pubmed',\n",
    "                            'use_pure':True}  # pure python mode is important for reading blobs of bytes\n",
    "write_size = 10000  # write to db every 10K rows\n",
    "\n",
    "\n",
    "\n",
    "path2embeddings = '/project2/jevans/brendan/pubmed_data_processing/validation_sets/jneurophysiol_vs_neuroimage_results/'\n",
    "\n",
    "# these are in single csv files - not spread across multiple chunk files\n",
    "'''\n",
    "#BERT approaches:\n",
    "embedding_names = ['scibert__longtoken_mean.csv',  # does not include [CLS]\n",
    "                   'scibert__tokenwise_mean.csv',  # does not include [CLS]\n",
    "                   'scibert__cls.csv',\n",
    "                   'vanilla__longtoken_mean.csv',  # does not include [CLS]\n",
    "                   'vanilla__tokenwise_mean.csv', # does not include [cls]\n",
    "                   'vanilla__cls.csv']\n",
    "\n",
    "# insert into mysql tables called:\n",
    "table_names = ['emb_scibert_longtokens_mean',\n",
    "                 'emb_scibert_tokens_mean',\n",
    "                 'emb_scibert_cls',\n",
    "                 'emb_bert_longtokens_mean', \n",
    "                 'emb_bert_tokens_mean',  \n",
    "                 'emb_bert_cls']\n",
    "'''\n",
    "#word2vec approaches\n",
    "embedding_names = ['w2v_pubmed.csv',\n",
    "                  'w2v_pubmed_pmc.csv',\n",
    "                  'w2v_wikipedia_pubmed_pmc.csv']\n",
    "# insert into mysql tables called:\n",
    "table_names = ['emb_w2v_pm',\n",
    "              'emb_w2v_pm_pmc',\n",
    "              'emb_w2v_wiki_pm_pmc']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating table emb_scibert_longtokens_mean...\n",
      "creating table emb_scibert_tokens_mean...\n",
      "creating table emb_scibert_cls...\n",
      "creating table emb_bert_longtokens_mean...\n",
      "creating table emb_bert_tokens_mean...\n",
      "creating table emb_bert_cls...\n"
     ]
    }
   ],
   "source": [
    "for table_name in table_names:\n",
    "    \n",
    "    print('creating table {}...'.format(table_name))\n",
    "\n",
    "    try:\n",
    "        db = mysql.connect(**client_config)\n",
    "        sql = '''CREATE TABLE {}\n",
    "                (pmid int NOT NULL,\n",
    "                embedding BLOB NOT NULL,\n",
    "                PRIMARY KEY (pmid))'''.format(table_name)\n",
    "        cursor = db.cursor()\n",
    "        cursor.execute(sql)\n",
    "        cursor.close()\n",
    "        db.commit()\n",
    "        db.close()\n",
    "    except Exception as e:\n",
    "        # table is probably already available\n",
    "        print('Warning during table creation:   {}'.format(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_db(entries, table_name):\n",
    "    db = mysql.connect(**client_config)\n",
    "    sql = '''INSERT INTO {} (pmid,embedding)\n",
    "             VALUES (%s, %s)\n",
    "             ON DUPLICATE KEY UPDATE\n",
    "             pmid=values(pmid), embedding=values(embedding)'''.format(table_name)\n",
    "    cursor = db.cursor()\n",
    "    cursor.executemany(sql, entries)\n",
    "    cursor.close()\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insert into db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserting scibert__longtoken_mean.csv into emb_scibert_longtokens_mean table...\n",
      "dumping to db\n",
      "dumping to db\n",
      "dumping to db\n",
      "elapsed: 42.83782410621643\n",
      "\n",
      "inserting scibert__tokenwise_mean.csv into emb_scibert_tokens_mean table...\n",
      "dumping to db\n",
      "dumping to db\n",
      "dumping to db\n",
      "elapsed: 44.003576040267944\n",
      "\n",
      "inserting scibert__cls.csv into emb_scibert_cls table...\n",
      "dumping to db\n",
      "dumping to db\n",
      "dumping to db\n",
      "elapsed: 40.23398995399475\n",
      "\n",
      "inserting vanilla__longtoken_mean.csv into emb_bert_longtokens_mean table...\n",
      "dumping to db\n",
      "dumping to db\n",
      "dumping to db\n",
      "elapsed: 45.219751834869385\n",
      "\n",
      "inserting vanilla__tokenwise_mean.csv into emb_bert_tokens_mean table...\n",
      "dumping to db\n",
      "dumping to db\n",
      "dumping to db\n",
      "elapsed: 39.46491026878357\n",
      "\n",
      "inserting vanilla__cls.csv into emb_bert_cls table...\n",
      "dumping to db\n",
      "dumping to db\n",
      "dumping to db\n",
      "elapsed: 38.50262999534607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for table_name, embedding_name in zip(table_names, embedding_names):\n",
    "    \n",
    "    print('inserting {} into {} table...'.format(embedding_name, table_name))\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = []\n",
    "    with open(path2embeddings + embedding_name) as f:\n",
    "        csvreader = csv.reader(f, delimiter=' ')\n",
    "        # note: there is no header in these csv files\n",
    "        \n",
    "        for idx, row in enumerate(csvreader):\n",
    "            pmid = int(row[0])\n",
    "            \n",
    "            embedding_blob = np.array(json.loads(row[1])).tobytes(\n",
    "                                                        order='C')\n",
    "            data.append((pmid, embedding_blob))\n",
    "            \n",
    "            if len(data) > write_size:  # write to db intermittently\n",
    "                print('dumping to db')\n",
    "                write_data_to_db(data, table_name) # csv -> mysql\n",
    "                data = [] # reset data  after writing\n",
    "    \n",
    "    write_data_to_db(data, table_name)  # empty the buffer in case it has some rows leftover\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('elapsed: {}'.format(end_time - start_time))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
