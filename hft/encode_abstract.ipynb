{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import *\n",
    "\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ne.g. (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\\n          (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased'),\\n          (RobertaModel, RobertaTokenizer, 'roberta-base')]\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = RobertaModel\n",
    "tokenizer_class = RobertaTokenizer\n",
    "pretrained_weights = 'roberta-base'  # todo roberta-large\n",
    "\n",
    "#model_class = BertModel\n",
    "#tokenizer_class = BertTokenizer\n",
    "#pretrained_weights = 'bert-base-uncased'\n",
    "\n",
    "'''\n",
    "e.g. (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
    "          (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased'),\n",
    "          (RobertaModel, RobertaTokenizer, 'roberta-base')]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights,\n",
    "                                    output_hidden_states=True,\n",
    "                                   output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = '''Representation and analysis of complex biological and \n",
    "engineered systems as directed networks is useful for understanding \n",
    "their global structure/function organization. Enrichment of network motifs, \n",
    "which are over-represented subgraphs in real networks, can be used for topological\n",
    "analysis. Because counting network motifs is computationally expensive, only\n",
    "characterization of 3- to 5-node motifs has been previously reported.\n",
    "In this study we used a supercomputer to analyze cyclic motifs made of 3–20 nodes \n",
    "for 6 biological and 3 technological networks. Using tools from statistical physics, we developed a theoretical framework for characterizing the ensemble of cyclic motifs in real networks. We have identified a generic property of real complex networks, antiferromagnetic organization, which is characterized by minimal directional coherence of edges along cyclic subgraphs, such that consecutive links tend to have opposing direction. As a consequence, we find that the lack of directional coherence in cyclic motifs leads to depletion in feedback loops, where the number of nodes affected by feedback loops appears to be at a local minimum compared with surrogate shuffled networks. This topology provides more dynamic stability in large networks.'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. tokens: ['Represent', 'ation', 'Ġand', 'Ġanalysis', 'Ġof', 'Ġcomplex', 'Ġbiological', 'Ġand', 'Ġ', 'Ċ', 'engine', 'ered', 'Ġsystems', 'Ġas', 'Ġdirected', 'Ġnetworks', 'Ġis', 'Ġuseful', 'Ġfor', 'Ġunderstanding', 'Ġ', 'Ċ', 'their', 'Ġglobal', 'Ġstructure', '/', 'function', 'Ġorganization', '.', 'ĠEn', 'rich', 'ment', 'Ġof', 'Ġnetwork', 'Ġmotif', 's', ',', 'Ġ', 'Ċ', 'which', 'Ġare', 'Ġover', '-', 'represented', 'Ġsub', 'graph', 's', 'Ġin', 'Ġreal', 'Ġnetworks', ',', 'Ġcan', 'Ġbe', 'Ġused', 'Ġfor', 'Ġtop', 'ological', 'Ċ', 'analysis', '.', 'ĠBecause', 'Ġcounting', 'Ġnetwork', 'Ġmotif', 's', 'Ġis', 'Ġcomput', 'ationally', 'Ġexpensive', ',', 'Ġonly', 'Ċ', 'character', 'ization', 'Ġof', 'Ġ3', '-', 'Ġto', 'Ġ5', '-', 'node', 'Ġmotif', 's', 'Ġhas', 'Ġbeen', 'Ġpreviously', 'Ġreported', '.', 'Ċ', 'In', 'Ġthis', 'Ġstudy', 'Ġwe', 'Ġused', 'Ġa', 'Ġsuper', 'computer', 'Ġto', 'Ġanalyze', 'Ġcycl', 'ic', 'Ġmotif', 's', 'Ġmade', 'Ġof', 'Ġ3', 'âĢĵ', '20', 'Ġnodes', 'Ġ', 'Ċ', 'for', 'Ġ6', 'Ġbiological', 'Ġand', 'Ġ3', 'Ġtechnological', 'Ġnetworks', '.', 'ĠUsing', 'Ġtools', 'Ġfrom', 'Ġstatistical', 'Ġphysics', ',', 'Ġwe', 'Ġdeveloped', 'Ġa', 'Ġtheoretical', 'Ġframework', 'Ġfor', 'Ġcharacter', 'izing', 'Ġthe', 'Ġensemble', 'Ġof', 'Ġcycl', 'ic', 'Ġmotif', 's', 'Ġin', 'Ġreal', 'Ġnetworks', '.', 'ĠWe', 'Ġhave', 'Ġidentified', 'Ġa', 'Ġgeneric', 'Ġproperty', 'Ġof', 'Ġreal', 'Ġcomplex', 'Ġnetworks', ',', 'Ġant', 'ifer', 'rom', 'agnetic', 'Ġorganization', ',', 'Ġwhich', 'Ġis', 'Ġcharacterized', 'Ġby', 'Ġminimal', 'Ġdirectional', 'Ġco', 'herence', 'Ġof', 'Ġedges', 'Ġalong', 'Ġcycl', 'ic', 'Ġsub', 'graph', 's', ',', 'Ġsuch', 'Ġthat', 'Ġconsecutive', 'Ġlinks', 'Ġtend', 'Ġto', 'Ġhave', 'Ġopposing', 'Ġdirection', '.', 'ĠAs', 'Ġa', 'Ġconsequence', ',', 'Ġwe', 'Ġfind', 'Ġthat', 'Ġthe', 'Ġlack', 'Ġof', 'Ġdirectional', 'Ġco', 'herence', 'Ġin', 'Ġcycl', 'ic', 'Ġmotif', 's', 'Ġleads', 'Ġto', 'Ġdepletion', 'Ġin', 'Ġfeedback', 'Ġloops', ',', 'Ġwhere', 'Ġthe', 'Ġnumber', 'Ġof', 'Ġnodes', 'Ġaffected', 'Ġby', 'Ġfeedback', 'Ġloops', 'Ġappears', 'Ġto', 'Ġbe', 'Ġat', 'Ġa', 'Ġlocal', 'Ġminimum', 'Ġcompared', 'Ġwith', 'Ġsurrogate', 'Ġshuff', 'led', 'Ġnetworks', '.', 'ĠThis', 'Ġtop', 'ology', 'Ġprovides', 'Ġmore', 'Ġdynamic', 'Ġstability', 'Ġin', 'Ġlarge', 'Ġnetworks', \".'\"]\n",
      "\n",
      "2. token_ids tensor: tensor([[    0, 28588,  1258,     8,  1966,     9,  2632, 12243,     8,  1437,\n",
      "         50118, 23403,  3215,  1743,    25,  3660,  4836,    16,  5616,    13,\n",
      "          2969,  1437, 50118, 25017,   720,  3184,    73, 35435,  1651,     4,\n",
      "          2271,  5691,  1757,     9,  1546, 32847,    29,     6,  1437, 50118,\n",
      "          5488,    32,    81,    12, 26716,  2849, 44143,    29,    11,   588,\n",
      "          4836,     6,    64,    28,   341,    13,   299,  9779, 50118, 31116,\n",
      "             4,  3047, 10581,  1546, 32847,    29,    16, 44316, 29688,  3214,\n",
      "             6,   129, 50118, 23375,  1938,     9,   155,    12,     7,   195,\n",
      "            12, 46840, 32847,    29,    34,    57,  1433,   431,     4, 50118,\n",
      "          1121,    42,   892,    52,   341,    10,  2422, 36327,     7, 11526,\n",
      "         13030,   636, 32847,    29,   156,     9,   155,  2383,   844, 32833,\n",
      "          1437, 50118,  1990,   231, 12243,     8,   155,  9874,  4836,     4,\n",
      "          8630,  3270,    31, 17325, 17759,     6,    52,  2226,    10, 26534,\n",
      "          7208,    13,  2048,  2787,     5, 12547,     9, 13030,   636, 32847,\n",
      "            29,    11,   588,  4836,     4,   166,    33,  2006,    10, 14569,\n",
      "          1038,     9,   588,  2632,  4836,     6,  9876, 14087,  5638, 44650,\n",
      "          1651,     6,    61,    16, 17407,    30,  9865, 40535,  1029, 40584,\n",
      "             9, 15716,   552, 13030,   636,  2849, 44143,    29,     6,   215,\n",
      "            14,  3396,  5678,  3805,     7,    33,  9375,  2698,     4,   287,\n",
      "            10, 15180,     6,    52,   465,    14,     5,  1762,     9, 40535,\n",
      "          1029, 40584,    11, 13030,   636, 32847,    29,  3315,     7, 39309,\n",
      "            11,  6456, 37482,     6,   147,     5,   346,     9, 32833,  2132,\n",
      "            30,  6456, 37482,  2092,     7,    28,    23,    10,   400,  3527,\n",
      "          1118,    19, 28247, 30573,  1329,  4836,     4,   152,   299,  4383,\n",
      "          1639,    55,  6878,  5443,    11,   739,  4836,   955,     2]])\n",
      "shape of last hidden states: torch.Size([1, 249, 768])\n"
     ]
    }
   ],
   "source": [
    "# set all outputs to on\n",
    "model.output_hidden_states=True\n",
    "\n",
    "# Tokenize input text:\n",
    "input_ids = torch.tensor([tokenizer.encode(abstract, add_special_tokens=True)]) \n",
    "\n",
    "# Equivalently, breaking down the steps of tokenization:\n",
    "input_tokens = tokenizer.tokenize(abstract)\n",
    "print(\"1. tokens: {}\".format(input_tokens))\n",
    "print()\n",
    "print(\"2. token_ids tensor: {}\".format(input_ids))\n",
    "\n",
    "# generate hidden activation from input tokens:\n",
    "with torch.no_grad():\n",
    "    last_hidden_state = model(input_ids)[0]\n",
    "    print(\"shape of last hidden states: {}\".format(np.shape(last_hidden_states)))\n",
    "    all_hidden_states, all_attentions = model(input_ids)[-2:]\n",
    "    #print(\"all hidden: {}\".format(np.shape(all_hidden_states)))\n",
    "    #print(\" all attentions: {}\".format(np.shape(all_attentions)))\n",
    "    \n",
    "    #print(last_hidden_states[0][0][0])\n",
    "    #print(all_hidden_states[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "torch.Size([1, 249, 768])\n"
     ]
    }
   ],
   "source": [
    "# number of hidden_states:\n",
    "print(len(all_hidden_states))\n",
    "\n",
    "# size of one layer\n",
    "print(all_hidden_states[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0121)\n",
      "tensor(0.0121)\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(last_hidden_state[0][0][0])\n",
    "print(all_hidden_states[12][0][0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
