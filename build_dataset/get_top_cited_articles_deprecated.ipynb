{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "# play with some sql selections to make sure things are working\n",
    "\n",
    "from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "import time\n",
    "import mysql.connector as mysql\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import Row, StructType, StructField, IntegerType, StringType\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "#import mysql.connector as mysql   # import gc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "#import igraph\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import cairocffi as cairo\n",
    "\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import statsmodels.api as sm  # for kdemultivariate \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_ARGS = \"--driver-class-path file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar --jars file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "\n",
    "db_name = 'test_pubmed'  # db name collisons? https://stackoverflow.com/questions/14011968/user-cant-access-a-database\n",
    "table_name = 'abstracts' # 'abstracts'\n",
    "\n",
    "url = \"jdbc:mysql://localhost:3306/{}?useUnicode=true&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=America/Chicago\".format(db_name)  # mysql runs on port 3306\n",
    "\n",
    "\n",
    "data_dir = '/project2/jevans/brendan/open_citation/'  # this is 2019\n",
    "open_cite_path = data_dir + 'open_citation_collection_2019-04.csv'\n",
    "metadata_path = data_dir + 'icite_metadata_2019-04.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing spark\n",
      "[('spark.repl.local.jars', 'file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar'), ('spark.driver.port', '43411'), ('spark.jars', '/home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar'), ('spark.driver.host', 'midway2-bigmem01.rcc.local'), ('spark.executor.id', 'driver'), ('spark.app.name', 'pyspark-shell'), ('spark.app.id', 'local-1564697399390'), ('spark.rdd.compress', 'True'), ('spark.driver.extraClassPath', 'file:///home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.driver.memory', '28G'), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "print('initializing spark')\n",
    "# init spark\n",
    "conf = SparkConf()\n",
    "conf = (conf.setMaster('local[*]')\n",
    "       .set('spark.driver.memory','28G')\n",
    "       .set(\"spark.jars\", \"/home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar\"))        \n",
    "'''\n",
    ".set('spark.executor.memory','1G')  # 20\n",
    ".set('spark.driver.memory','1G')   # 40\n",
    ".set('spark.driver.maxResultSize','500M')  #.set('spark.storage.memoryFraction',0))  # this setting is now a legacy option\n",
    ".set('spark.python.worker.reuse', 'false')\n",
    ".set('spark.python.worker.memory','512m')\n",
    ".set('spark.executor.cores','1'))\n",
    "'''\n",
    "sc = SparkContext(conf=conf)\n",
    "#sc.addJar('home/brendanchambers/my_resources/mysql-connector-java-8.0.16/mysql-connector-java-8.0.16.jar')  # temp\n",
    "spark = SparkSession(sc)  # don't need this for vanilla RDDs\n",
    "\n",
    "print(sc._conf.getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most cited papers within a given year,\n",
    "\n",
    "# embed the within a single space\n",
    "\n",
    "# plot the changing hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- citing: string (nullable = true)\n",
      " |-- referenced: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- pmid: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- human: string (nullable = true)\n",
      " |-- animal: string (nullable = true)\n",
      " |-- molcel: string (nullable = true)\n",
      " |-- xcoord: string (nullable = true)\n",
      " |-- ycoord: string (nullable = true)\n",
      " |-- approx_potential_translate: string (nullable = true)\n",
      " |-- cited_by_clin: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- journal: string (nullable = true)\n",
      " |-- article: string (nullable = true)\n",
      " |-- is_clinical: string (nullable = true)\n",
      " |-- estimated: string (nullable = true)\n",
      " |-- relative_citation_ratio: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load in open citation dataset\n",
    "\n",
    "edgelist = spark.read.csv(open_cite_path, header=True)\n",
    "metadata = spark.read.csv(metadata_path, header=True)\n",
    "\n",
    "# see schema\n",
    "edgelist.printSchema()\n",
    "metadata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many articles are cited every year?\n",
    "\n",
    "# how many are published (in the pubmed db)?\n",
    "\n",
    "# ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1990']\n",
      "N papers: 410220\n",
      "elapsed: 0.41318607330322266\n"
     ]
    }
   ],
   "source": [
    "# for a single year\n",
    "\n",
    "START_YEAR = 2008  # inclusive\n",
    "END_YEAR = 2009  # exclusive\n",
    "year_set = []\n",
    "for year in range(START_YEAR, END_YEAR):\n",
    "    year_set.append(str(year))\n",
    "print(year_set)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "papers_for_year = metadata.filter(metadata.year.isin(year_set)).cache()\n",
    "\n",
    "N_papers = papers_for_year.count()\n",
    "print(\"N papers: {}\".format(N_papers))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"elapsed: {}\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "articles count: 109952\n"
     ]
    }
   ],
   "source": [
    "# for many years\n",
    "\n",
    "\n",
    "# for estimating running time -\n",
    "# 10 s to get the pmids after caching (1959)\n",
    "# 136 s to get the initial articles df (1960)  # todo use sql w indexing to speed this up?\n",
    "\n",
    "\n",
    "\n",
    "start_year = 1959  # inclusive\n",
    "end_year = 2020  # exclusive\n",
    "articles_data = {}\n",
    "\n",
    "for year in range(start_year, end_year):\n",
    "    start_time = time.time()\n",
    "    print(year)\n",
    "    year_set = [year]  # todo is there function that doesn't require a list\n",
    "    \n",
    "    articles = metadata.filter(metadata.year.isin(year_set)).cache()\n",
    "    \n",
    "    articles_count = articles.count()\n",
    "    print(\"articles count: {}\".format(articles_count))\n",
    "    \n",
    "    year_pmids = articles.rdd.map(lambda row: row['pmid']).collect()\n",
    "    articles.unpersist()\n",
    "    \n",
    "    # get cited articles\n",
    "    year_pmids_broadcast = sc.broadcast(year_pmids)\n",
    "    citation_count = edgelist.where(edgelist['referenced'].isin(year_pmids_broadcast.value))\\\n",
    "            .select('referenced').groupBy('referenced').count().collect()\n",
    "    \n",
    "    citations_rdd = citation_count.rdd.cache()\n",
    "    cited_pmids = citations_rdd.map(lambda row: row['referenced']).collect()\n",
    "    cited_count = citations_rdd.map(lambda row: row['count']).collect()\n",
    "    \n",
    "    # todo write data\n",
    "    \n",
    "    articles_data[year] = {'N': articles_count,\n",
    "                           'published_pmids': year_pmids,\n",
    "                           'cited_pmids': cited_pmids,\n",
    "                           'citation_count': cited_count}\n",
    "    \n",
    "    target_filename = 'json/articles_data_{}.json'.format(year)\n",
    "    with open(target_filename,'w') as f:\n",
    "        json.dump(articles_data[year], f)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"elapsed: {}\".format(end_time - start_time))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export text data from top citations by year"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
